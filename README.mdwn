
Build Boot Bot
==============

For old Linux Kernel architectures like mips, arm, sparc etc it is
very difficult to test the variety of machines supported. Most of
the time the kernel is only compile tested and nobody knows if those
machines still work. 

This project tries to eliminate that by creating an automatic build infrastructure
for kernels of those architectures. The input is a git tree, a commit id,
an architecture and machine definition (Some machines may be big
AND little endian e.g. the SiByte 1250 Swarm)

Then the central job control api offers this job to one or more
builders which then spawn a predefined docker container and build
the tree. Resulting kernel, logfile, config etc are uploaded to
a WebDAV directory.

Another job which depends on the first then fetches the resulting
kernel tar.gz and prepares a boot environment. This will be heavy
machine specific. Some machines to have IPMI/LOM/BMC style methods
for powering and resetting, most old machines wont. 

So running/booting a machine will consist of preparing an nfs root
by e.g. cloning an logical volume, putting the kernel modules in
there, exporing per nfs, preparing the DHCP/bootp/RARP environment
and powering up the machine for example by a WLAN power plug. 

The machine will then boot until the PROM at which the boot bot
will take over and log all serial output, and possibly
emulating commands in prom to boot from network.

It will be possible to add a list of tests which should be run 
in the resulting NFS directory e.g. sometimes it will make
sense to create a filesystem, unpack a rootfs and switch to
the filesystem as NFS does not tend to break non coherency as
much as SCSI DMA does.

Or we could run parts of the LTP automatically, compile stuff,
unpack a kernel etc.

Afterwards the machine will be powered off and the nfs boot environment
will be destroyed.

All output will then be send back to the WebDAV directory.

Buildbot
========

The buildbot uses docker to schedule builds. So the user you are running
with needs to have docker permissions. Most of the time this should be 
able to achieve with "adduser docker flo" for example.

For installation use this:

	sudo apt-get install gnupg2 lsb-release jq libhttp-dav-perl \
		libmodern-perl-perl libgetopt-long-descriptive-perl \
		libwww-perl libhttp-dav-perl curl libfile-slurp-perl \
		libjson-perl
	
	echo deb [arch=amd64] https://download.docker.com/linux/debian buster stable \
		| sudo tee -a /etc/apt/sources.list
	
	curl -fsSL https://download.docker.com/linux/ubuntu/gpg \
		| sudo apt-key add -
	
	sudo apt-get update
	
	sudo apt-get -fuy install docker-ce 
	
	sudo adduser $(whoami) docker

At this point you will need to log out and login again otherwise
you wont be able to use docker as user.

To prepare the docker images needed to build kernels 

	git clone git://github.com/flohoff/build-boot-bot
	cd build-boot-bot
	buildbot/scripts/preparedocker

Then run 

	buildbot/buildbot \
		--api https://nuc.dynamic.uucico.de/bbb \
		-c docker:bbb/buster \
		-b $PWD/buildbot/scripts/build

As soon as there are build jobs you should see builds beein scheduled.

Command line client
===================

	bbb --api https://nuc.dynamic.uucico.de/bbb --status --job 1 --raw

	bbb --api https://nuc.dynamic.uucico.de/bbb \
		--submit --build \
		-r docker:bbb/buster \
		-v commit=HEAD \
		-v gittree=git://git.kernel.org/pub/scm/linux/kernel/git/mips/linux.git \
		-v machine=sb1250-defconfig

Build Boot Bot API
==================

The central REST API does not have a clue about what type of jobs are
beeing submitted, scheduled, canceled etc. These are opaque jobs which
carry dependencies on other jobs, requirements for "runners" to fullfill
and variables e.g. the jobs input values.

The API just keeps hold of which jobs exists, what their dependencies
are, and who and when somebody claimed it to run the job.

Everything else if out of scope of the central Job API.

Installation
------------

	git clone git://github.com/flohoff/build-boot-bot

	apt-get install \
		libfile-slurp-perl libmojo-server-fastcgi-perl \
		libmojolicious-perl libjson-perl \
		postgresql libdbd-pg-perl \
		apache2 libapache2-mod-fcgid 

	service postgres start

	su - postgres -c "createuser -s bbbapi"
	su - postgres -c "createdb -O bbbapi bbb"
	su - postgres -c "psql -c \"alter user bbbapi encrypted password 'foobarbaz';\""

	psql -h localhost -U bbbapi bbb -f api/sql/schema.sql

	cat <<EOF >api/config.json
	{
	"dbhost": "dbi:Pg:dbname=bbb;host=localhost",
	"dbuser": "bbbapi",
	"dbpass": "foobarbaz"
	}
	EOF

Edit apache2 site config and add this for the API:

	ScriptAlias /bbb/ /home/<user>/build-boot-bot/api/bbbapi/
        <Location /bbb/>
                Options ExecCGI
                SetHandler fcgid-script
		Require all granted
        </Location>     

And add a directory with WebDAV for your artifacts:;

	<Directory "/var/www/html/artifacts/">
		Require all granted
		Dav On
		Options Indexes FollowSymLinks MultiViews
	</Directory>

And create the directory
	
	mkdir /var/www/html/artifacts
	chown www-data.www-data /var/www/html/artifacts

Enable the fcgid, dav and dav_fs module:

	a2enmod fcgid
	a2enmod dav
	a2enmod dav_fs
	service apache2 restart
	
Job status values
=================

	submitted	- Submitted by client
	waiting 	- Waiting for client to claim
	claimed 	- Claimed by client
	ok		- Returned by client ok
	failed		- Returned by client failed
	cancelled	- Canceled - Parent failed
	dependency	- Waiting for dependency

REST API 
=========

Job Status
----------

	curl -XGET https://nuc.dynamic.uucico.de/bbb/v1/job/1/status | jq

	{
	  "claimed": "p4",
	  "id": 1,
	  "processing": "2020-09-05 19:55:27.701255",
	  "queued": "2020-09-05 18:38:29.626586",
	  "requirements": [
	    "docker:bbb/buster",
	    "mips"
	  ],
	  "result": null,
	  "returned": null,
	  "type": "build",
	  "uid": "flo",
	  "variables": {
	    "commit": "HEAD",
	    "gittree": "git://git.kernel.org/pub/scm/linux/kernel/git/mips/linux.git",
	    "machine": "sb1250-bigendian"
          }	
        }

Filter jobs
-----------

To find a job matching my capabilities:

	curl -XPOST -d '{ "type": "build", "capabilities": [ "docker:bbb/buster", "mipsel" ] }' \
		https://nuc.dynamic.uucico.de/bbb/v1/job/filter

Returns a list ob jobs.

Claim a job
-----------
To claim a job for processing

	curl -XGET https://nuc.dynamic.uucico.de/bbb/v1/job/1/claim/p4

Returns a status with ok/fail and the job claimed

	{
	  "status": "fail"
	  "error": "Job already claimed",
	  "job": {
 		 ... 
	    }
	  },
	}

Submit job
==========

To submit a new job:

	curl -XPOST -d '{ "type": "build", "requirements": [ "docker:bbb/buster" ], "variables": { "foo": "bar" }}' https://nuc.dynamic.uucico.de/bbb/v1/job/submit

Returns a status ok/fail, the job id and the job json itself


Submit job
==========
	bbb-submit
		Arguments
			-j -> jobtype - string
			-r -> requirement - may be listed as often as wanted
			-v -> key/value variable setting
			-d -> depends on jobid $foo
		Returns
			jobid

	bbb-submit \
		-j build \
		-r docker:buster \
		-r arch:mips \
		-v gittree=git://git.kernel.org/pub/scm/linux/kernel/git/mips/linux.git \
		-v commit=HEAD \
		-v machine=sb1250-bigendian

	Returns the job number

	bbb-submit \
		-j boot \
		-r sb1250* \
		-d ${jobid} \
		-v kernel=${jobid}

Bootbot
=======

For your dhcp/tftp setup
	
	apt-get install isc-dhcp-server tftpd-hpa

Other dependencies

	apt-get install libclone-perl

DHCP, TFTP and NFS
------------------

For machines doing network based booting via bootp/tftp/nfs
configure the isc-dhcp-server to hand out bootp/dhcp leases to
clients. You might want to hardcode the clients booting like this. 
IP addresses must match your /etc/network/interfaces, nfs config,
machine definition and possibly your PROM settings of your DUT.

	subnet 172.27.72.0 netmask 255.255.255.0 {
		range 172.27.72.64 172.27.72.128;

		option routers 172.27.72.1;
		next-server 172.27.72.1;

		option domain-name-servers 192.168.178.1;
		option domain-name "b1i.zz.de";
	}

	host p6064 {
		hardware ethernet 00:40:bc:06:00:40;
		fixed-address 172.27.72.32;
		filename "/p6064/vmlinux";
	}

For tftp make shure you chroot into the tftp directory to
make bootfile paths shorter. A lot of PROMs/Firmware has
serious constraints for length of tftp filename.

So make shure it is started with

		TFTP_OPTIONS="--secure -v"

For Debian 10/Buster this is configured in /etc/default/tftpd-hpa.
For certain broken PROMs for example the Decstation you will also
need to restrict the ports with --port-range


GPIO usage
----------
If you are using some kind of GPIOs to trigger external reset/power for your target add this udev rule to make shure you
are allowed to trigger the gpios as user:

	addgroup gpio
	cat <<EOF >/etc/udev/rules.d/10-gpio.rules
	KERNEL=="gpio*", MODE="0660", GROUP="gpio"
	EOF


Ideas and Todos
===============

- Buildbot should autodiscover docker images
- Timeout for jobs
- API should send DAV url for storage of the artifacts
- Notifications?
  - How to aggregate build/boot in ONE notification?
  - How to send notification if only a parent fails?
  - Notification on success/failure
- cmdline client
  - download artifacts of job
  - cancel job
